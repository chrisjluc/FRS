{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN-H1 4-22-2016</h1>\n",
    "\n",
    "<strong>Abstract</strong>\n",
    "Implementing the CNN-H1 using NN2 described in the paper: http://arxiv.org/pdf/1509.00244v1.pdf. \n",
    "\n",
    "<strong>Implementation</strong>\n",
    "<ul>\n",
    "<li>Normalizes images to 230x230 centered around the facial features and then takes the middle 165X120 portion</li>\n",
    "<li>Validation split of 15%</li>\n",
    "<li>fc6 layer has a length of 512</li>\n",
    "<li>Noise with width 15</li>\n",
    "<li>Number of people is 600</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is enabled with initial size: 98.0% of memory, CuDNN 3007)\n"
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import dlib\n",
    "import os\n",
    "import fnmatch\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.spatial import distance\n",
    "from scipy.misc import imrotate\n",
    "\n",
    "from skimage import io\n",
    "from skimage.color import rgb2grey\n",
    "from skimage.transform import resize\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_lfw_path = '../data/lfw'\n",
    "data_path = '../data'\n",
    "norm_shape = 230, 230\n",
    "input_shape = 165, 120\n",
    "noise_width = 15\n",
    "num_people = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Loading Files</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_face_to_file_path_dict():\n",
    "    face_to_file_paths_dict = {}\n",
    "    \n",
    "    for root, dirnames, filenames in os.walk(data_lfw_path):\n",
    "        for dirname in dirnames:\n",
    "            if dirname not in face_to_file_paths_dict:\n",
    "                face_to_file_paths_dict[dirname] = []\n",
    "            directory_path = os.path.join(data_lfw_path, dirname)\n",
    "            for filename in os.listdir(directory_path):\n",
    "                if filename.endswith(\".jpg\"):\n",
    "                    face_to_file_paths_dict[dirname].append(os.path.join(directory_path, filename))\n",
    "                            \n",
    "    return face_to_file_paths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_face_to_file_paths_descending_list(face_to_file_paths_dict):\n",
    "    return sorted(face_to_file_paths_dict.items(), key=lambda x: len(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "face_to_file_paths_dict = get_face_to_file_path_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "face_to_file_paths_list = get_face_to_file_paths_descending_list(face_to_file_paths_dict)[:num_people]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper Functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_centroid(points):\n",
    "    x = [p[0] for p in points]\n",
    "    y = [p[1] for p in points]\n",
    "    return (sum(x) / len(points), sum(y) / len(points))\n",
    "    \n",
    "def get_facial_feature_points(im, face):\n",
    "    points = np.array([[p.x, p.y] for p in predictor(im, face).parts()])\n",
    "    facial_feature_points = [None for i in range(5)]\n",
    "    \n",
    "    # Left and right eyes, nose, left and right points of the mouth\n",
    "    facial_feature_points[0] = get_centroid(np.concatenate((points[37:39], points[40:42])))\n",
    "    facial_feature_points[1] = get_centroid(np.concatenate((points[43:45], points[46:48])))\n",
    "    facial_feature_points[2] = points[30]\n",
    "    facial_feature_points[3] = points[48]\n",
    "    facial_feature_points[4] = points[54]\n",
    "    \n",
    "    return np.array(facial_feature_points)\n",
    "\n",
    "def get_facial_landmark_points(im, face, w=.6, eye_scale=.05):\n",
    "    points = np.array([[p.x, p.y] for p in predictor(im, face).parts()])\n",
    "    landmarks = [None for _ in range(9)]\n",
    "    \n",
    "    p = get_centroid([points[19], points[24]])\n",
    "    dx = p[0] - points[51][0]\n",
    "    dy = p[1] - points[51][1]\n",
    "\n",
    "    # Left\n",
    "    landmarks[0] = points[19]\n",
    "    \n",
    "    p = get_centroid(points[40:42])\n",
    "    landmarks[1] = (p[0] - dx * eye_scale, p[1] - dy * eye_scale)\n",
    "    \n",
    "    landmarks[2] = (points[2][0] * (1 - w) + w * points[31][0],\n",
    "                      points[2][1] * (1 - w) + w * points[31][1])\n",
    "    \n",
    "    # Centre\n",
    "    p = get_centroid([points[19], points[24]])\n",
    "    landmarks[3] = (p[0] + dx * .1, p[1] + dy * .1)\n",
    "    landmarks[4] = points[29]\n",
    "    landmarks[5] = points[51]\n",
    "    \n",
    "    # Right\n",
    "    landmarks[6] = points[24]\n",
    "    \n",
    "    p = get_centroid(points[46:48])\n",
    "    landmarks[7] = (p[0] - dx * eye_scale, p[1] - dy * eye_scale)\n",
    "    \n",
    "    landmarks[8] = (points[14][0] * (1 - w) + w * points[35][0],\n",
    "                      points[14][1] * (1 - w) + w * points[35][1])\n",
    "    \n",
    "    return np.array(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image_window(image, size, point):\n",
    "    \"\"\"\n",
    "    Assume image is grey image\n",
    "    \"\"\"\n",
    "    top = int(point[1] - size[0] / 2)\n",
    "    left = int(point[0] - size[1] / 2)\n",
    "    return image[top:top + size[0], left:left + size[1]]\n",
    "\n",
    "\n",
    "def get_most_centre_face(image):\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    image_centre = width / 2, height / 2\n",
    "    most_centre_face = None\n",
    "    most_centre_dist = sys.maxint\n",
    "    \n",
    "    detected_faces = detector(img, 1)\n",
    "    for d in detected_faces:\n",
    "        face_centre = (d.right() + d.left()) / 2, (d.bottom() + d.top()) / 2\n",
    "        centre_dist = distance.euclidean(image_centre, face_centre)\n",
    "        if centre_dist < most_centre_dist:\n",
    "            most_centre_dist = centre_dist\n",
    "            most_centre_face = d\n",
    "            \n",
    "    return most_centre_face\n",
    "\n",
    "def calculate_rotation(points):\n",
    "    \"\"\"\n",
    "    Return degrees of how much the face is rotated off the vertical axis\n",
    "    \"\"\"\n",
    "    u = points[3]\n",
    "    v = points[5]\n",
    "    return math.degrees(np.arctan((v[0] - u[0]) / (v[1] - u[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reflection(image):\n",
    "    return np.array([list(reversed(row)) for row in image])\n",
    "\n",
    "def partition(image, top_left, rows, cols):\n",
    "    return np.array([row[top_left[1]:top_left[1] + cols] for row in image[top_left[0]:top_left[0] + rows]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_coords(coords, resize_shape, original_shape):\n",
    "    scale = np.array(resize_shape).astype(float) / np.array(original_shape)\n",
    "    coords[:,0] = coords[:,0] * scale[1]\n",
    "    coords[:,1] = coords[:,1] * scale[0]\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_noise_image(image, coords, width):\n",
    "    \"\"\"\n",
    "    Apply random gaussian generated values\n",
    "    and distribute them on gaussian distributed square\n",
    "    centered on the coordinates passed in for the image\n",
    "    \"\"\"\n",
    "    \n",
    "    noise = np.zeros((image.shape[0], image.shape[1]))\n",
    "    for coord in coords:\n",
    "        # Convert coordinates to rows / columns\n",
    "        apply_noise_at_point(coord[1], coord[0], noise, width)\n",
    "    return np.clip(image + noise, 0, 1)\n",
    "\n",
    "def apply_noise_at_point(x, y, noise, width):\n",
    "    \"\"\"\n",
    "    Generate a block with a single random value placed at the center\n",
    "    Apply the Gaussian filter with std of 4\n",
    "    Place it on the noise array at the appropriate coordinates\n",
    "    \n",
    "    x represents the rows\n",
    "    y represents the cols\n",
    "    \"\"\"\n",
    "    \n",
    "    block = np.zeros((width, width))\n",
    "    block[width / 2, width / 2] = np.random.normal()\n",
    "    block = gaussian_filter(block, sigma=4)\n",
    "    \n",
    "    x -= width / 2\n",
    "    y -= width / 2\n",
    "    \n",
    "    x_end = min(noise.shape[0] - x, block.shape[0])\n",
    "    x_start =  max(0, -x)\n",
    "\n",
    "    y_end = min(noise.shape[1] - y, block.shape[1])\n",
    "    y_start = max(0, -y)\n",
    "\n",
    "    noise[max(0, x):x+block.shape[0], max(0, y):y+block.shape[1]] = block[x_start:x_end,y_start:y_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_noise(image, coords):\n",
    "    return get_random_noise_image(image, coords, noise_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Pre-Processing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(os.path.join(data_path, 'shape_predictor_68_face_landmarks.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_by_class = [[io.imread(f) for f in x[1]] for x in face_to_file_paths_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(images_by_class)):\n",
    "    for j in range(len(images_by_class[i])):\n",
    "        try:\n",
    "            img = images_by_class[i][j]\n",
    "            # Rotate image\n",
    "            face = get_most_centre_face(img)\n",
    "            landmark_points = get_facial_landmark_points(img, face)\n",
    "            angle = calculate_rotation(landmark_points)\n",
    "            img = imrotate(img, -angle)\n",
    "\n",
    "            # Normalize and cropping image by centering around the facial features\n",
    "            face = get_most_centre_face(img)\n",
    "            landmark_points = get_facial_landmark_points(img, face)\n",
    "            feature_points = get_facial_feature_points(img, face)\n",
    "\n",
    "            face_height = face.bottom() - face.top()\n",
    "            face_width = face.right() - face.left()\n",
    "            scale_height = .5\n",
    "            scale_width = .5\n",
    "\n",
    "            left = max(0, face.left() - int(face_width * scale_width))\n",
    "            top = max(0, face.top() - int(face_height * scale_height))\n",
    "            right = min(img.shape[1], face.right() + int(face_width * scale_width))\n",
    "            bottom = min(img.shape[0], face.bottom() + int(face_height * scale_height))\n",
    "\n",
    "            img = img[top:bottom, left:right]\n",
    "            landmark_points[:,0] -= left\n",
    "            landmark_points[:,1] -= top\n",
    "            feature_points[:,0] -= left\n",
    "            feature_points[:,1] -= top\n",
    "\n",
    "            # Resizing image and scaling facial landmark and feature points\n",
    "            img = rgb2grey(img)            \n",
    "            img_shape = img.shape\n",
    "            img = resize(img, norm_shape)\n",
    "\n",
    "            scale = np.array(norm_shape).astype(float) / np.array(img_shape)\n",
    "            landmark_points[:,0] = landmark_points[:,0] * scale[1]\n",
    "            landmark_points[:,1] = landmark_points[:,1] * scale[0]\n",
    "            feature_points[:,0] = feature_points[:,0] * scale[1]\n",
    "            feature_points[:,1] = feature_points[:,1] * scale[0]\n",
    "\n",
    "            images_by_class[i][j] = (img, landmark_points, feature_points)\n",
    "        except TypeError:\n",
    "             images_by_class[i][j] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_by_class = [[image for image in images if image is not None] for images in images_by_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create reflection with reflected coordinates\n",
    "for i in range(len(images_by_class)):\n",
    "    for j in range(len(images_by_class[i])):\n",
    "        im, landmarks, features = images_by_class[i][j]\n",
    "        new_features = [(im.shape[1] - p[0], p[1]) for p in features]\n",
    "        new_landmarks = [(im.shape[1] - p[0], p[1]) for p in landmarks]\n",
    "        new_landmarks[:3], new_landmarks[6:] = new_landmarks[6:], new_landmarks[:3]\n",
    "        images_by_class[i].append((reflection(im), new_landmarks, new_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Doubling the images and applying random gaussian noise\n",
    "for i in range(len(images_by_class)):\n",
    "    images_by_class[i] += images_by_class[i][:]\n",
    "    images_by_class[i] = [(apply_noise(im, features), landmarks) \n",
    "                          for im, landmarks, features in images_by_class[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get image of size 165x120, similar to the paper\n",
    "for i in range(len(images_by_class)):\n",
    "    images_by_class[i] = [get_image_window(im, input_shape, landmarks[4])\n",
    "                          for im, landmarks in images_by_class[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((165, 120), 26668)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter([image.shape\n",
    "            for images in images_by_class \n",
    "            for image in images if image.shape == input_shape\n",
    "            ]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = [image \n",
    "            for images in images_by_class \n",
    "            for image in images \n",
    "            if image.shape == input_shape\n",
    "            ]\n",
    "y_train = [images[0] \n",
    "            for images in enumerate(images_by_class) \n",
    "            for image in images[1] \n",
    "            if image.shape == input_shape\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zipped = np.array(zip(X_train, y_train))\n",
    "np.random.shuffle(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.array([x[0] for x in zipped])\n",
    "y_train = np.array([x[1] for x in zipped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 1, input_shape[0], input_shape[1])\n",
    "Y_train = np_utils.to_categorical(y_train, len(images_by_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training and Validation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN2(shape, nb_classes, nb_fc6):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', input_shape=shape))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    #Layer 4\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    #Layer 5\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(AveragePooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(nb_fc6))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_input_shape = (1, input_shape[0], input_shape[1])\n",
    "nb_fc6 = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22667 samples, validate on 4001 samples\n",
      "Epoch 1/10\n",
      "22667/22667 [==============================] - 738s - loss: 5.8960 - acc: 0.0713 - val_loss: 5.7173 - val_acc: 0.0855\n",
      "Epoch 2/10\n",
      "22667/22667 [==============================] - 738s - loss: 5.7443 - acc: 0.0781 - val_loss: 5.6357 - val_acc: 0.0855\n",
      "Epoch 3/10\n",
      "22667/22667 [==============================] - 738s - loss: 5.4763 - acc: 0.0895 - val_loss: 5.1808 - val_acc: 0.1157\n",
      "Epoch 4/10\n",
      "22667/22667 [==============================] - 738s - loss: 4.8810 - acc: 0.1390 - val_loss: 4.7553 - val_acc: 0.1790\n",
      "Epoch 5/10\n",
      "22667/22667 [==============================] - 739s - loss: 3.9866 - acc: 0.2499 - val_loss: 3.4491 - val_acc: 0.3164\n",
      "Epoch 6/10\n",
      "22667/22667 [==============================] - 738s - loss: 2.6569 - acc: 0.4326 - val_loss: 2.3956 - val_acc: 0.4724\n",
      "Epoch 7/10\n",
      "22667/22667 [==============================] - 739s - loss: 1.4839 - acc: 0.6437 - val_loss: 1.7597 - val_acc: 0.5974\n",
      "Epoch 8/10\n",
      "22667/22667 [==============================] - 738s - loss: 0.7324 - acc: 0.8056 - val_loss: 0.9179 - val_acc: 0.7586\n",
      "Epoch 9/10\n",
      "22667/22667 [==============================] - 739s - loss: 0.3623 - acc: 0.8957 - val_loss: 0.5116 - val_acc: 0.8673\n",
      "Epoch 10/10\n",
      "22667/22667 [==============================] - 738s - loss: 0.1992 - acc: 0.9407 - val_loss: 0.6186 - val_acc: 0.8548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6ec9eb6690>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN2(model_input_shape, num_people, nb_fc6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10, \n",
    "        show_accuracy=True, verbose=1, shuffle=True, validation_split=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "open('models/CNN-H1_epoch-10.json', 'w').write(json_string)\n",
    "model.save_weights('models/CNN-H1_epoch-10.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22667 samples, validate on 4001 samples\n",
      "Epoch 1/10\n",
      "22667/22667 [==============================] - 739s - loss: 0.0538 - acc: 0.9846 - val_loss: 0.1868 - val_acc: 0.9630\n",
      "Epoch 2/10\n",
      "22667/22667 [==============================] - 739s - loss: 0.0241 - acc: 0.9944 - val_loss: 0.1838 - val_acc: 0.9640\n",
      "Epoch 3/10\n",
      "22667/22667 [==============================] - 739s - loss: 0.0155 - acc: 0.9961 - val_loss: 0.1915 - val_acc: 0.9663\n",
      "Epoch 4/10\n",
      "22667/22667 [==============================] - 739s - loss: 0.0158 - acc: 0.9961 - val_loss: 0.1836 - val_acc: 0.9670\n",
      "Epoch 5/10\n",
      "22667/22667 [==============================] - 739s - loss: 0.0127 - acc: 0.9971 - val_loss: 0.1860 - val_acc: 0.9658\n",
      "Epoch 6/10\n",
      "22667/22667 [==============================] - 739s - loss: 0.0107 - acc: 0.9974 - val_loss: 0.1880 - val_acc: 0.9685\n",
      "Epoch 7/10\n",
      "22667/22667 [==============================] - 739s - loss: 0.0104 - acc: 0.9978 - val_loss: 0.1921 - val_acc: 0.9668\n",
      "Epoch 8/10\n",
      "22667/22667 [==============================] - 739s - loss: 0.0089 - acc: 0.9984 - val_loss: 0.1960 - val_acc: 0.9673\n",
      "Epoch 9/10\n",
      "22667/22667 [==============================] - 739s - loss: 0.0091 - acc: 0.9982 - val_loss: 0.1940 - val_acc: 0.9668\n",
      "Epoch 10/10\n",
      "22667/22667 [==============================] - 739s - loss: 0.0083 - acc: 0.9981 - val_loss: 0.1964 - val_acc: 0.9678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6c9cf6db10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001))\n",
    "model.fit(X_train, Y_train, batch_size=32, nb_epoch=10,\n",
    "        show_accuracy=True, verbose=1, shuffle=True, validation_split=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_string = model.to_json()\n",
    "open('models/CNN-H1_epoch-20.json', 'w').write(json_string)\n",
    "model.save_weights('models/CNN-H1_epoch-20.h5', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
